{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BouTWaBkWfW",
        "outputId": "1cd73566-7734-47da-ca2a-258419a28966"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwhlmfFOkayf"
      },
      "outputs": [],
      "source": [
        "PATH = '/content/drive/MyDrive/Dataset/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jZeQBj_Ikcfc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vlaqoQ7fkeSs"
      },
      "outputs": [],
      "source": [
        "positive_df = pd.read_csv('positive_sample_7500_1.csv', header=None)\n",
        "negative_df = pd.read_csv('negative_sample_7500_1.csv', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qWkX8eB6kgSU"
      },
      "outputs": [],
      "source": [
        "def load_protein_sequences(file_path):\n",
        "    protein_dict = {}\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split('\\t')\n",
        "            if len(parts) == 2:\n",
        "                protein_dict[parts[0]] = parts[1]\n",
        "    return protein_dict\n",
        "protein_sequences = load_protein_sequences('protein_sequences.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "48CiuEjbkich"
      },
      "outputs": [],
      "source": [
        "positive_df_2 = pd.DataFrame([positive_df[1],positive_df[0]]).transpose()\n",
        "negative_df_2 = pd.DataFrame([negative_df[1],negative_df[0]]).transpose()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4P6GOa0nkl_j"
      },
      "outputs": [],
      "source": [
        "positive_df = pd.concat([positive_df,positive_df_2])\n",
        "negative_df = pd.concat([negative_df,negative_df_2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VmIlUX_Fkn6D"
      },
      "outputs": [],
      "source": [
        "positive_df[2] = 1\n",
        "negative_df[2] = 0\n",
        "data = pd.concat([positive_df, negative_df],ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rA2wtfrWkp7D"
      },
      "outputs": [],
      "source": [
        "data = data.sample(frac=1,random_state=42).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rj1ppMBXktPO"
      },
      "outputs": [],
      "source": [
        "data[0] = [protein_sequences[x] for x in data[0]]\n",
        "data[1] = [protein_sequences[x] for x in data[1]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vspjEu2Akvx4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, concatenate\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5s78l-Tsky8N"
      },
      "outputs": [],
      "source": [
        "sequences1 = data[0].values  # Amino acid sequences of protein 1\n",
        "sequences2 = data[1].values  # Amino acid sequences of protein 2\n",
        "labels = data[2].values     # Interaction labels (0 or 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7TZbcQtwk0m8"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(char_level=True)  # Tokenize at character level\n",
        "tokenizer.fit_on_texts(sequences1 + sequences2)\n",
        "\n",
        "# Convert sequences to numerical tokens\n",
        "encoded_sequences1 = tokenizer.texts_to_sequences(sequences1)\n",
        "encoded_sequences2 = tokenizer.texts_to_sequences(sequences2)\n",
        "\n",
        "# Pad sequences to a fixed length (choose an appropriate maxlen)\n",
        "maxlen = 250  # Example max length, adjust as needed\n",
        "padded_sequences1 = pad_sequences(encoded_sequences1, maxlen=maxlen, padding='post')\n",
        "padded_sequences2 = pad_sequences(encoded_sequences2, maxlen=maxlen, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mygNymrck3PI"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1  # Vocabulary size for embedding layer\n",
        "\n",
        "# Input layers for protein sequences\n",
        "input1 = Input(shape=(maxlen,))\n",
        "input2 = Input(shape=(maxlen,))\n",
        "\n",
        "# Embedding layers to represent amino acids as vectors\n",
        "embedding_layer = Embedding(vocab_size, 128)  # 128-dimensional embeddings\n",
        "embedded_sequences1 = embedding_layer(input1)\n",
        "embedded_sequences2 = embedding_layer(input2)\n",
        "\n",
        "# LSTM layers with additional hidden layers\n",
        "lstm_layer1 = LSTM(64, return_sequences=True)  # First LSTM layer with return_sequences=True\n",
        "lstm_layer2 = LSTM(64, return_sequences=True)  # Second LSTM layer with return_sequences=True\n",
        "lstm_layer3 = LSTM(64)  # Third LSTM layer\n",
        "\n",
        "lstm_output1 = lstm_layer1(embedded_sequences1)\n",
        "lstm_output1 = lstm_layer2(lstm_output1)  # Pass output of first LSTM to the second\n",
        "lstm_output1 = lstm_layer3(lstm_output1)  # Pass output of second LSTM to the third\n",
        "\n",
        "lstm_output2 = lstm_layer1(embedded_sequences2)\n",
        "lstm_output2 = lstm_layer2(lstm_output2)  # Pass output of first LSTM to the second\n",
        "lstm_output2 = lstm_layer3(lstm_output2)  # Pass output of second LSTM to the third\n",
        "\n",
        "# Concatenate LSTM outputs\n",
        "merged = concatenate([lstm_output1, lstm_output2])\n",
        "\n",
        "# Dense layers for classification\n",
        "output = Dense(1, activation='sigmoid')(merged)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=[input1, input2], outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgQLaWoRlI_9",
        "outputId": "85d01d86-d500-4c54-bb12-bbdf1412e8ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1\n",
            "Epoch 1/100\n",
            "\u001b[1m 8/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m23s\u001b[0m 12s/step - accuracy: 0.5159 - loss: 0.6928"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Define the number of folds\n",
        "k = 5  # You can adjust this value\n",
        "\n",
        "# Create KFold object\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=42)  # Shuffle and set random_state for reproducibility\n",
        "\n",
        "# Lists to store evaluation results\n",
        "accuracies = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1_scores = []\n",
        "\n",
        "# Iterate through folds\n",
        "for fold, (train_index, test_index) in enumerate(kf.split(padded_sequences1)):  # Assuming padded_sequences1 and padded_sequences2 have the same length\n",
        "    print(f\"Fold {fold + 1}\")\n",
        "\n",
        "    # Split data into training and testing sets for this fold\n",
        "    train_sequences1_fold, test_sequences1_fold = padded_sequences1[train_index], padded_sequences1[test_index]\n",
        "    train_sequences2_fold, test_sequences2_fold = padded_sequences2[train_index], padded_sequences2[test_index]\n",
        "    train_labels_fold, test_labels_fold = labels[train_index], labels[test_index]\n",
        "\n",
        "    # Train the model on this fold's training data\n",
        "    model.fit([train_sequences1_fold, train_sequences2_fold], train_labels_fold, epochs=100, batch_size=2500)\n",
        "\n",
        "    # Evaluate the model on this fold's testing data\n",
        "    predictions = model.predict([test_sequences1_fold, test_sequences2_fold])\n",
        "    predicted_labels_fold = (predictions > 0.5).astype(int)\n",
        "\n",
        "    accuracy = accuracy_score(test_labels_fold, predicted_labels_fold)\n",
        "    precision = precision_score(test_labels_fold, predicted_labels_fold)\n",
        "    recall = recall_score(test_labels_fold, predicted_labels_fold)\n",
        "    f1 = f1_score(test_labels_fold, predicted_labels_fold)\n",
        "\n",
        "    accuracies.append(accuracy)\n",
        "    precisions.append(precision)\n",
        "    recalls.append(recall)\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "    print(f\"Fold {fold + 1} - Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ah0qsH98lX9J",
        "outputId": "59f00982-45e5-483d-e9d4-2efed52ef041"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Accuracy: 0.9452\n",
            "Average Precision: 0.9463\n",
            "Average Recalls: 0.9442\n",
            "Average F1 Scores: 0.9452\n"
          ]
        }
      ],
      "source": [
        "# Calculate average accuracy and loss across all folds\n",
        "average_accuracy = np.mean(accuracies)\n",
        "average_precision = np.mean(precisions)\n",
        "average_recalls = np.mean(recalls)\n",
        "average_f1_scores = np.mean(f1_scores)\n",
        "\n",
        "print(f\"Average Accuracy: {average_accuracy:.4f}\")\n",
        "print(f\"Average Precision: {average_precision:.4f}\")\n",
        "print(f\"Average Recalls: {average_recalls:.4f}\")\n",
        "print(f\"Average F1 Scores: {average_f1_scores:.4f}\")\n",
        "model.save('5fold_3layer.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m934/934\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.9963 - loss: 0.0157\n",
            "Loaded model accuracy: 0.9961\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "\n",
        "# Load the model\n",
        "\n",
        "loaded_model = load_model('protein_interaction_model.h5')\n",
        "\n",
        "\n",
        "# Verify that the loaded model works\n",
        "\n",
        "loss, accuracy = loaded_model.evaluate([padded_sequences1, padded_sequences2], labels)\n",
        "\n",
        "print(f'Loaded model accuracy: {accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\adakd\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 14 variables whereas the saved optimizer has 26 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 25ms/step - accuracy: 0.7216 - loss: 2.1635\n",
            "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 25ms/step\n",
            "Loaded model accuracy: 0.7207\n",
            "Precision: 0.7168\n",
            "Recall: 0.7297\n",
            "F1 Score: 0.7232\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from joblib import dump, load\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load positive and negative data\n",
        "positive_df = pd.read_csv('positive_2500.csv', header=None)\n",
        "negative_df = pd.read_csv('negative_2500.csv', header=None)\n",
        "\n",
        "def load_protein_sequences(file_path):\n",
        "    protein_dict = {}\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split('\\t')\n",
        "            if len(parts) == 2:\n",
        "                protein_dict[parts[0]] = parts[1]\n",
        "    return protein_dict\n",
        "\n",
        "# Load protein sequences\n",
        "protein_sequences = load_protein_sequences('protein_sequences.txt')\n",
        "\n",
        "# Prepare the data\n",
        "positive_df_2 = pd.DataFrame([positive_df[1], positive_df[0]]).transpose()\n",
        "negative_df_2 = pd.DataFrame([negative_df[1], negative_df[0]]).transpose()\n",
        "positive_df = pd.concat([positive_df, positive_df_2])\n",
        "negative_df = pd.concat([negative_df, negative_df_2])\n",
        "\n",
        "positive_df[2] = 1\n",
        "negative_df[2] = 0\n",
        "data = pd.concat([positive_df, negative_df], ignore_index=True)\n",
        "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "data[0] = [protein_sequences[x] for x in data[0]]\n",
        "data[1] = [protein_sequences[x] for x in data[1]]\n",
        "\n",
        "sequences1 = data[0].values  # Amino acid sequences of protein 1\n",
        "sequences2 = data[1].values  # Amino acid sequences of protein 2\n",
        "labels = data[2].values       # Interaction labels (0 or 1)\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer(char_level=True)  # Tokenize at character level\n",
        "tokenizer.fit_on_texts(sequences1 + sequences2)\n",
        "\n",
        "# Convert sequences to numerical tokens\n",
        "encoded_sequences1 = tokenizer.texts_to_sequences(sequences1)\n",
        "encoded_sequences2 = tokenizer.texts_to_sequences(sequences2)\n",
        "\n",
        "# Pad sequences to a fixed length\n",
        "maxlen = 100  # Example max length, adjust as needed\n",
        "test_sequences1 = pad_sequences(encoded_sequences1, maxlen=maxlen, padding='post')\n",
        "test_sequences2 = pad_sequences(encoded_sequences2, maxlen=maxlen, padding='post')\n",
        "\n",
        "# Load the model\n",
        "loaded_model = load('5fold_3layer_PPI_Model.joblib')\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = loaded_model.evaluate([test_sequences1, test_sequences2], labels)\n",
        "\n",
        "# Make predictions\n",
        "predictions = loaded_model.predict([test_sequences1, test_sequences2])\n",
        "predicted_labels = (predictions > 0.5).astype(int)  # Convert probabilities to binary labels\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(labels, predicted_labels)\n",
        "recall = recall_score(labels, predicted_labels)\n",
        "f1 = f1_score(labels, predicted_labels)\n",
        "\n",
        "# Print the results\n",
        "print(f'Loaded model accuracy: {accuracy:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
